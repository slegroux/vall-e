{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from valle.modules.embedding import SinePositionalEmbedding, TokenEmbedding\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenized Text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 184])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 128\n",
    "batch_size = 3\n",
    "max_length = 184\n",
    "x = torch.randint(vocab_size, (batch_size, max_length))\n",
    "x_lens = torch.tensor([163,184,152])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio features\n",
    "batch_size = 3\n",
    "encodec_size = 8\n",
    "max_y_lens = 775\n",
    "y = torch.randn((batch_size, max_y_lens, encodec_size))*800\n",
    "y_lens = torch.tensor([770,775,768])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pad_mask(lengths: torch.Tensor, max_len: int = 0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      lengths:\n",
    "        A 1-D tensor containing sentence lengths.\n",
    "      max_len:\n",
    "        The length of masks.\n",
    "    Returns:\n",
    "      Return a 2-D bool tensor, where masked positions\n",
    "      are filled with `True` and non-masked positions are\n",
    "      filled with `False`.\n",
    "\n",
    "    >>> lengths = torch.tensor([1, 3, 2, 5])\n",
    "    >>> make_pad_mask(lengths)\n",
    "    tensor([[False,  True,  True,  True,  True],\n",
    "            [False, False, False,  True,  True],\n",
    "            [False, False,  True,  True,  True],\n",
    "            [False, False, False, False, False]])\n",
    "    \"\"\"\n",
    "    assert lengths.ndim == 1, lengths.ndim\n",
    "    max_len = max(max_len, lengths.max())\n",
    "    n = lengths.size(0)\n",
    "\n",
    "    expaned_lengths = torch.arange(max_len).expand(n, max_len).to(lengths)\n",
    "\n",
    "    return expaned_lengths >= lengths.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True])\n"
     ]
    }
   ],
   "source": [
    "x_mask = make_pad_mask(x_lens)\n",
    "print(x_mask[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_model: int,\n",
    "        vocab_size: int,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dim_model = dim_model\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "        self.word_embeddings = nn.Embedding(self.vocab_size, self.dim_model)\n",
    "\n",
    "    @property\n",
    "    def weight(self) -> torch.Tensor:\n",
    "        return self.word_embeddings.weight\n",
    "\n",
    "    def embedding(self, index: int) -> torch.Tensor:\n",
    "        return self.word_embeddings.weight[index : index + 1]\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        X = self.word_embeddings(x)\n",
    "        X = self.dropout(X)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {'dim_model': 128, 'vocab_size':128, 'dropout':0.0}\n",
    "t = TokenEmbedding(**cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 184]) torch.Size([3, 184, 128])\n"
     ]
    }
   ],
   "source": [
    "embedded = t(x)\n",
    "print(x.shape, embedded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinePositionalEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self, dim_model: int, dropout: float = 0.0, scale: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim_model = dim_model\n",
    "        self.x_scale = math.sqrt(dim_model) if scale else 1.0\n",
    "        self.alpha = nn.Parameter(torch.ones(1))\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        self.reverse = False\n",
    "        self.pe = None\n",
    "        self.extend_pe(torch.tensor(0.0).expand(1, 4000))\n",
    "\n",
    "    def extend_pe(self, x):\n",
    "        \"\"\"Reset the positional encodings.\"\"\"\n",
    "        if self.pe is not None:\n",
    "            if self.pe.size(1) >= x.size(1):\n",
    "                if self.pe.dtype != x.dtype or self.pe.device != x.device:\n",
    "                    self.pe = self.pe.to(dtype=x.dtype, device=x.device)\n",
    "                return\n",
    "        pe = torch.zeros(x.size(1), self.dim_model)\n",
    "        if self.reverse:\n",
    "            position = torch.arange(\n",
    "                x.size(1) - 1, -1, -1.0, dtype=torch.float32\n",
    "            ).unsqueeze(1)\n",
    "        else:\n",
    "            position = torch.arange(\n",
    "                0, x.size(1), dtype=torch.float32\n",
    "            ).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, self.dim_model, 2, dtype=torch.float32)\n",
    "            * -(math.log(10000.0) / self.dim_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.pe = pe.to(device=x.device, dtype=x.dtype).detach()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        self.extend_pe(x)\n",
    "        output = x.unsqueeze(-1) if x.ndim == 2 else x\n",
    "        output = output * self.x_scale + self.alpha * self.pe[:, : x.size(1)]\n",
    "        return self.dropout(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 184, 128])\n"
     ]
    }
   ],
   "source": [
    "dim_model = 128\n",
    "pos = SinePositionalEmbedding(dim_model=dim_model)\n",
    "positioned = pos(embedded)\n",
    "print(positioned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 2.4003e-01, -1.1978e-01,  6.9747e-01,  ...,  7.1296e-01,\n",
      "          -1.2161e+00,  7.1315e-02],\n",
      "         [-6.4140e-01,  4.0792e-01,  2.2685e-01,  ..., -6.5513e-01,\n",
      "          -1.3306e+00,  1.5161e+00],\n",
      "         [ 1.3071e+00,  6.3669e-01, -1.7437e-01,  ...,  1.6955e-01,\n",
      "          -1.2495e+00, -1.7102e-01],\n",
      "         ...,\n",
      "         [-4.2391e-01, -1.5363e-03, -3.8377e-01,  ..., -1.8966e+00,\n",
      "           4.3150e-01, -2.7159e+00],\n",
      "         [-1.1792e-01,  2.4326e-02,  8.5095e-01,  ...,  9.6202e-01,\n",
      "          -9.1284e-01,  1.3664e+00],\n",
      "         [ 1.2822e-01,  4.0712e-01, -4.7922e-01,  ..., -9.1959e-01,\n",
      "          -1.0553e-01, -7.6964e-01]],\n",
      "\n",
      "        [[ 4.9819e-01, -1.2775e-01,  1.1892e+00,  ...,  9.5521e-01,\n",
      "          -9.6966e-02, -2.2252e+00],\n",
      "         [ 3.4125e-01,  1.3086e+00,  2.9771e-01,  ...,  2.6346e-01,\n",
      "           5.6558e-01, -4.2933e-01],\n",
      "         [-1.6659e+00,  1.6146e-01, -1.7946e+00,  ..., -1.3131e+00,\n",
      "           2.4257e+00,  5.1549e-01],\n",
      "         ...,\n",
      "         [-1.6473e+00, -1.0983e+00, -7.2494e-01,  ...,  2.4630e-01,\n",
      "          -1.1034e+00,  3.7739e-01],\n",
      "         [-1.0989e+00, -7.0409e-01,  2.0992e+00,  ...,  1.2328e+00,\n",
      "          -8.2743e-02,  1.7326e+00],\n",
      "         [-1.0118e+00, -1.2044e+00,  8.2808e-01,  ...,  9.1632e-01,\n",
      "           3.4755e-01, -1.1373e+00]],\n",
      "\n",
      "        [[-4.8950e-01,  2.9483e-01,  4.9377e-01,  ..., -1.2821e+00,\n",
      "          -1.4587e+00,  1.6015e+00],\n",
      "         [-7.7914e-01, -1.7655e+00, -6.9240e-01,  ...,  6.0224e-01,\n",
      "          -3.7715e-01,  4.7541e-02],\n",
      "         [-2.8270e+00,  7.1816e-03,  1.2480e+00,  ..., -6.9933e-01,\n",
      "           6.0553e-01,  1.8016e+00],\n",
      "         ...,\n",
      "         [-1.1378e+00,  9.3815e-01,  2.2543e-01,  ..., -1.7534e+00,\n",
      "           1.9840e+00,  1.0055e+00],\n",
      "         [-9.8057e-01,  2.1650e+00,  4.0104e-01,  ...,  1.4973e-02,\n",
      "          -1.1982e+00, -2.2685e-01],\n",
      "         [-9.6473e-01, -8.7786e-03, -2.9116e-01,  ...,  8.1580e-01,\n",
      "           1.0187e-01, -8.6451e-01]]])\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]])\n",
      "tensor([[[ 1032,   -45,   575,  ...,    78,  -448,   116],\n",
      "         [ -176,  2239,     7,  ...,  -381, -1074,  1045],\n",
      "         [  957,   176,   110,  ..., -1003,  1221,   868],\n",
      "         ...,\n",
      "         [    0,     0,     0,  ...,     0,     0,     0],\n",
      "         [    0,     0,     0,  ...,     0,     0,     0],\n",
      "         [    0,     0,     0,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  175,   -55,  1597,  ...,  -161,   380,  1447],\n",
      "         [  291,   454,    56,  ...,   692,  -814,   294],\n",
      "         [  715,  1215,    -5,  ...,   543,  -600,   827],\n",
      "         ...,\n",
      "         [ -500,  -207,    21,  ...,  -381,  1081,   266],\n",
      "         [   98,    15,   216,  ..., -1397,   359,  1247],\n",
      "         [  124,   567,  1259,  ..., -1200,  -667,   334]],\n",
      "\n",
      "        [[  270,   -29,   840,  ...,  -505,   206,  1017],\n",
      "         [ -210,     4,   449,  ...,  -936,  -242,  -714],\n",
      "         [ -118,   811,    59,  ..., -1625,   694,  -379],\n",
      "         ...,\n",
      "         [    0,     0,     0,  ...,     0,     0,     0],\n",
      "         [    0,     0,     0,  ...,     0,     0,     0],\n",
      "         [    0,     0,     0,  ...,     0,     0,     0]]]) torch.Size([3, 775, 8])\n"
     ]
    }
   ],
   "source": [
    "y_mask = make_pad_mask(y_lens)\n",
    "y_mask_int = y_mask.type(torch.int64)\n",
    "print(y_mask_int)\n",
    "codes = y.type(torch.int64) * (1 - y_mask_int.unsqueeze(dim=-1))\n",
    "print(codes, codes.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1032,   -45,   575,  -696,  -959,    78,  -448,   116],\n",
       "        [ -176,  2239,     7,   333,   -39,  -381, -1074,  1045],\n",
       "        [  957,   176,   110,  -136, -2441, -1003,  1221,   868],\n",
       "        [ 1454,   164, -2637,  -119,  -365,  -957,  -293,   813],\n",
       "        [-1610,   248,   414, -1230,   197,    71,   -52,   519]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codes[0,:5,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "valle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78912e90a8b6844e455c0b86093ceec57d8549ad8a3bfb9bbb11106c9e95be43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
